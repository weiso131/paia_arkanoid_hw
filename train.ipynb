{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# with open('C:/Users/weiso131/Desktop/paia2.4.5/resources/app.asar.unpacked/games/arkanoid/ml/graph/graph' + str(1) + '.pickle', 'rb') as f:\n",
    "#     data = pickle.load(f)\n",
    "#     for i in range(10):\n",
    "#         print(data[0][i].shape)\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84932, 1, 40, 100)\n",
      "(84932,)\n"
     ]
    }
   ],
   "source": [
    "x_data = []\n",
    "y_label = []\n",
    "\n",
    "for i in range(10000):\n",
    "    path = 'C:/Users/weiso131/Desktop/paia2.4.5/resources/app.asar.unpacked/games/arkanoid/ml/graph/graph' + str(i) + '.pickle'\n",
    "    if (os.path.exists(path)):\n",
    "        \n",
    "        with open(path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        graphs, label = data[0], data[1]\n",
    "        for graph in graphs:\n",
    "            x_data.append(np.array([graph]))\n",
    "            y_label.append(float(label))\n",
    "    else:\n",
    "        break\n",
    "\n",
    "x_data = np.array(x_data)\n",
    "y_label = np.array(y_label)\n",
    "print(x_data.shape)\n",
    "print(y_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "\n",
    "\n",
    "# 將NumPy陣列轉換為PyTorch張量\n",
    "x_data_tensor = torch.Tensor(x_data)  # 使用torch.Tensor默認轉換為float32\n",
    "y_label_tensor = torch.Tensor(y_label) \n",
    "\n",
    "# 創建TensorDataset\n",
    "dataset = TensorDataset(x_data_tensor, y_label_tensor)\n",
    "\n",
    "# 分割數據集為訓練集和驗證集，比如90%訓練，10%驗證\n",
    "train_size = int(0.9 * len(dataset))\n",
    "valid_size = len(dataset) - train_size\n",
    "train_dataset, valid_dataset = random_split(dataset, [train_size, valid_size])\n",
    "\n",
    "batch = 64\n",
    "\n",
    "# 創建DataLoader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch, shuffle=True)\n",
    "\n",
    "# 現在train_dataloader和valid_dataloader都已經準備好，可以在訓練和驗證過程中使用了\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 40, 100]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_dataloader:\n",
    "    print(x.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 設定裝置\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 定義CNN模型\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv_layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(in_channels=32, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            self._to_linear = self.conv_layer(torch.zeros(1, 1, 40, 100))\n",
    "            self._to_linear = torch.flatten(self._to_linear)\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Linear(len(self._to_linear), 1024),\n",
    "            nn.Dropout(0.7),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layer(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc_layer(x)\n",
    "        return x\n",
    "    \n",
    "# 定義NN模型\n",
    "class NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NN, self).__init__()\n",
    "\n",
    "        self._to_linear = torch.flatten(torch.zeros((1, 40, 100)))\n",
    "\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Linear(len(self._to_linear), 4096),\n",
    "            nn.Dropout(0.7),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4096, 1024),\n",
    "            nn.Dropout(0.7),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv_layer): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (9): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (10): ReLU()\n",
       "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc_layer): Sequential(\n",
       "    (0): Linear(in_features=6144, out_features=1024, bias=True)\n",
       "    (1): Dropout(p=0.7, inplace=False)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=1024, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CNN().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=10):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    min_val_loss = 1e9\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device, dtype=torch.float32), labels.to(device, dtype=torch.float32)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels.view(-1, 1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device, dtype=torch.float32), labels.to(device, dtype=torch.float32)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels.view(-1, 1))\n",
    "                running_loss += loss.item()\n",
    "        val_loss = running_loss / len(val_loader)\n",
    "\n",
    "        if (val_loss < min_val_loss):\n",
    "            print(\"save model\")\n",
    "            torch.save(model.state_dict(), \"checkpoint2.pth\")\n",
    "            min_val_loss = val_loss\n",
    "\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Train Loss: {train_loss}, Val Loss: {val_loss}')\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.title('Train vs Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model\n",
      "Epoch 1, Train Loss: 80.90052227834279, Val Loss: 49.70296954391594\n",
      "save model\n",
      "Epoch 2, Train Loss: 42.96852563036037, Val Loss: 33.788745263465366\n",
      "save model\n",
      "Epoch 3, Train Loss: 31.511565274673526, Val Loss: 25.04547821848016\n",
      "save model\n",
      "Epoch 4, Train Loss: 24.801741693408918, Val Loss: 21.919937119448097\n",
      "save model\n",
      "Epoch 5, Train Loss: 20.22736592871375, Val Loss: 17.708360815406742\n",
      "save model\n",
      "Epoch 6, Train Loss: 17.06445491782791, Val Loss: 14.964367414775648\n",
      "save model\n",
      "Epoch 7, Train Loss: 15.007019851796297, Val Loss: 14.559090392033857\n",
      "save model\n",
      "Epoch 8, Train Loss: 13.303308461400754, Val Loss: 11.96988334512352\n",
      "save model\n",
      "Epoch 9, Train Loss: 11.980064343009534, Val Loss: 11.622738744979515\n",
      "save model\n",
      "Epoch 10, Train Loss: 11.116134885364996, Val Loss: 11.088347456509009\n",
      "save model\n",
      "Epoch 11, Train Loss: 10.23248222023872, Val Loss: 9.80539796226903\n",
      "save model\n",
      "Epoch 12, Train Loss: 9.6930688706402, Val Loss: 8.995603971911553\n",
      "save model\n",
      "Epoch 13, Train Loss: 9.132266985123128, Val Loss: 8.595392453043084\n",
      "save model\n",
      "Epoch 14, Train Loss: 8.697098148617286, Val Loss: 8.204593441540137\n",
      "save model\n",
      "Epoch 15, Train Loss: 8.2211317182086, Val Loss: 7.781372197588584\n",
      "save model\n",
      "Epoch 16, Train Loss: 7.953925757029067, Val Loss: 7.559210632080422\n",
      "save model\n",
      "Epoch 17, Train Loss: 7.698275307531636, Val Loss: 7.537112223474603\n",
      "save model\n",
      "Epoch 18, Train Loss: 7.468339570895399, Val Loss: 6.797332261738024\n",
      "Epoch 19, Train Loss: 7.262610423714546, Val Loss: 7.135661376150031\n",
      "save model\n",
      "Epoch 20, Train Loss: 7.015408649803704, Val Loss: 6.791343355537357\n",
      "save model\n",
      "Epoch 21, Train Loss: 6.822238303826943, Val Loss: 6.472651494176764\n",
      "save model\n",
      "Epoch 22, Train Loss: 6.6675240077733, Val Loss: 6.258325040788579\n",
      "Epoch 23, Train Loss: 6.556412011709173, Val Loss: 6.5005329067545725\n",
      "save model\n",
      "Epoch 24, Train Loss: 6.334259098643538, Val Loss: 6.108631458497586\n",
      "Epoch 25, Train Loss: 6.270067188430531, Val Loss: 6.6946208674208565\n",
      "save model\n",
      "Epoch 26, Train Loss: 6.156929488162117, Val Loss: 5.188974708542788\n",
      "Epoch 27, Train Loss: 6.007764426434888, Val Loss: 5.49606292229846\n",
      "Epoch 28, Train Loss: 5.967310424030575, Val Loss: 5.285580912927039\n",
      "save model\n",
      "Epoch 29, Train Loss: 5.781932697934586, Val Loss: 5.050138406287458\n",
      "Epoch 30, Train Loss: 5.705221100731375, Val Loss: 5.4905078142209165\n",
      "save model\n",
      "Epoch 31, Train Loss: 5.62987811814791, Val Loss: 5.043358109051123\n",
      "save model\n",
      "Epoch 32, Train Loss: 5.54985385340128, Val Loss: 4.82411619774381\n",
      "Epoch 33, Train Loss: 5.442775103437352, Val Loss: 5.162899223485387\n",
      "save model\n",
      "Epoch 34, Train Loss: 5.48010115503766, Val Loss: 4.555847191272822\n",
      "Epoch 35, Train Loss: 5.318146591904772, Val Loss: 4.909671806751337\n",
      "Epoch 36, Train Loss: 5.332018454404057, Val Loss: 4.591197120515924\n",
      "save model\n",
      "Epoch 37, Train Loss: 5.250908718747574, Val Loss: 4.5041775506241875\n",
      "Epoch 38, Train Loss: 5.185293440639224, Val Loss: 4.526716650876784\n",
      "save model\n",
      "Epoch 39, Train Loss: 5.107468629781172, Val Loss: 4.444354851443069\n",
      "Epoch 40, Train Loss: 5.077415934566673, Val Loss: 4.765068922724042\n",
      "save model\n",
      "Epoch 41, Train Loss: 5.01997897914264, Val Loss: 4.321165907651858\n",
      "Epoch 42, Train Loss: 4.981893143793529, Val Loss: 4.71595779666327\n",
      "save model\n",
      "Epoch 43, Train Loss: 4.9179153727687055, Val Loss: 4.307153912415182\n",
      "Epoch 44, Train Loss: 4.797134299258308, Val Loss: 4.42805360224014\n",
      "save model\n",
      "Epoch 45, Train Loss: 4.8250702588628025, Val Loss: 4.184461080938354\n",
      "Epoch 46, Train Loss: 4.752872624157861, Val Loss: 4.197429719724153\n",
      "save model\n",
      "Epoch 47, Train Loss: 4.764786825219956, Val Loss: 4.097129443534334\n",
      "save model\n",
      "Epoch 48, Train Loss: 4.64421190337656, Val Loss: 3.995525408508186\n",
      "save model\n",
      "Epoch 49, Train Loss: 4.61018278369345, Val Loss: 3.9653754028162562\n",
      "save model\n",
      "Epoch 50, Train Loss: 4.635057811856768, Val Loss: 3.9008848407214747\n",
      "Epoch 51, Train Loss: 4.579360868641522, Val Loss: 4.13205675612715\n",
      "Epoch 52, Train Loss: 4.550991543366819, Val Loss: 3.984052670629401\n",
      "Epoch 53, Train Loss: 4.531096966994856, Val Loss: 4.140052791824915\n",
      "save model\n",
      "Epoch 54, Train Loss: 4.507309600000102, Val Loss: 3.890107846797857\n",
      "Epoch 55, Train Loss: 4.451244262870884, Val Loss: 4.03455362463356\n",
      "Epoch 56, Train Loss: 4.416995798294514, Val Loss: 4.124782388371632\n",
      "save model\n",
      "Epoch 57, Train Loss: 4.375515317218573, Val Loss: 3.7578619189728473\n",
      "Epoch 58, Train Loss: 4.338624620637136, Val Loss: 3.7665536045131827\n",
      "Epoch 59, Train Loss: 4.282044866593812, Val Loss: 3.9478536360245897\n",
      "save model\n",
      "Epoch 60, Train Loss: 4.287079256249272, Val Loss: 3.7119166591113673\n",
      "Epoch 61, Train Loss: 4.281967401504517, Val Loss: 3.777116726215621\n",
      "Epoch 62, Train Loss: 4.235052764764889, Val Loss: 4.210122803100069\n",
      "Epoch 63, Train Loss: 4.244088086523272, Val Loss: 3.7253190529973885\n",
      "Epoch 64, Train Loss: 4.167129587728109, Val Loss: 3.9008931351783582\n",
      "Epoch 65, Train Loss: 4.162318463505063, Val Loss: 3.826890778720827\n",
      "Epoch 66, Train Loss: 4.120497379243124, Val Loss: 4.039052505242197\n",
      "save model\n",
      "Epoch 67, Train Loss: 4.090692806942194, Val Loss: 3.6288148702535414\n",
      "save model\n",
      "Epoch 68, Train Loss: 4.068790446365229, Val Loss: 3.597507953643799\n",
      "Epoch 69, Train Loss: 4.08324892750345, Val Loss: 3.665621160564566\n",
      "Epoch 70, Train Loss: 4.0072692810242145, Val Loss: 3.766793453603759\n",
      "save model\n",
      "Epoch 71, Train Loss: 3.9882289400659348, Val Loss: 3.491997446332659\n",
      "Epoch 72, Train Loss: 3.9842310317889416, Val Loss: 3.619990156109172\n",
      "save model\n",
      "Epoch 73, Train Loss: 3.937141523301352, Val Loss: 3.4570293892595103\n",
      "Epoch 74, Train Loss: 3.961679054902687, Val Loss: 3.4651191557260383\n",
      "save model\n",
      "Epoch 75, Train Loss: 3.9593072103157203, Val Loss: 3.3844936745507375\n",
      "Epoch 76, Train Loss: 3.896515572819251, Val Loss: 3.551769605256561\n",
      "Epoch 77, Train Loss: 3.846326411119565, Val Loss: 3.6485079123561546\n",
      "save model\n",
      "Epoch 78, Train Loss: 3.832312754108317, Val Loss: 3.3643242694381486\n",
      "Epoch 79, Train Loss: 3.861468107630518, Val Loss: 3.7773188899334214\n",
      "Epoch 80, Train Loss: 3.8536226456135387, Val Loss: 3.440540559309766\n",
      "save model\n",
      "Epoch 81, Train Loss: 3.7432139478467996, Val Loss: 3.3238804716812935\n",
      "Epoch 82, Train Loss: 3.8070743959817928, Val Loss: 3.326996662562951\n",
      "Epoch 83, Train Loss: 3.7937702037300527, Val Loss: 3.7916979368467976\n",
      "Epoch 84, Train Loss: 3.7354958350688343, Val Loss: 3.8565543477696584\n",
      "save model\n",
      "Epoch 85, Train Loss: 3.7282460957890273, Val Loss: 3.2591888366785264\n",
      "Epoch 86, Train Loss: 3.7372408695300754, Val Loss: 3.41364551262748\n",
      "Epoch 87, Train Loss: 3.6607223135656892, Val Loss: 3.2707506355486418\n",
      "Epoch 88, Train Loss: 3.6344492577109877, Val Loss: 3.350266521138356\n",
      "Epoch 89, Train Loss: 3.6540668174312705, Val Loss: 3.924640737081829\n",
      "Epoch 90, Train Loss: 3.622733162836051, Val Loss: 3.5378295353480746\n",
      "save model\n",
      "Epoch 91, Train Loss: 3.602268058784836, Val Loss: 3.258198851033261\n",
      "Epoch 92, Train Loss: 3.583620440111998, Val Loss: 3.4080812240901746\n",
      "Epoch 93, Train Loss: 3.5514939047801444, Val Loss: 3.298183688543793\n",
      "save model\n",
      "Epoch 94, Train Loss: 3.565748582325221, Val Loss: 3.25509423376026\n",
      "Epoch 95, Train Loss: 3.5669640259762687, Val Loss: 3.385163026644771\n",
      "save model\n",
      "Epoch 96, Train Loss: 3.4993510872749107, Val Loss: 3.234968715144279\n",
      "save model\n",
      "Epoch 97, Train Loss: 3.4826681307668967, Val Loss: 3.201417140046457\n",
      "save model\n",
      "Epoch 98, Train Loss: 3.545660656366388, Val Loss: 3.160760429568757\n",
      "Epoch 99, Train Loss: 3.4535455185000368, Val Loss: 3.2577626839616243\n",
      "Epoch 100, Train Loss: 3.4762939528940113, Val Loss: 3.2666431772977784\n",
      "Epoch 101, Train Loss: 3.4728362242048254, Val Loss: 3.256637407424755\n",
      "save model\n",
      "Epoch 102, Train Loss: 3.434099391614044, Val Loss: 3.117535074402515\n",
      "Epoch 103, Train Loss: 3.3996399180160903, Val Loss: 3.2537266508977214\n",
      "Epoch 104, Train Loss: 3.3926536786506367, Val Loss: 3.4553552328195787\n",
      "Epoch 105, Train Loss: 3.4339867318524475, Val Loss: 3.146544611543641\n",
      "Epoch 106, Train Loss: 3.4178766537909726, Val Loss: 3.2938070781248854\n",
      "Epoch 107, Train Loss: 3.41140698309224, Val Loss: 3.15209442421906\n",
      "Epoch 108, Train Loss: 3.3442405609904973, Val Loss: 3.254038677179724\n",
      "Epoch 109, Train Loss: 3.3940950350781365, Val Loss: 3.29569067004928\n",
      "Epoch 110, Train Loss: 3.3390448796699235, Val Loss: 3.3750852664610496\n",
      "save model\n",
      "Epoch 111, Train Loss: 3.298966087257513, Val Loss: 3.0969350813026715\n",
      "save model\n",
      "Epoch 112, Train Loss: 3.333426228128218, Val Loss: 3.0929271623604278\n",
      "Epoch 113, Train Loss: 3.3184457481655616, Val Loss: 3.173829934202639\n",
      "Epoch 114, Train Loss: 3.2714953372668023, Val Loss: 3.49862926078022\n",
      "save model\n",
      "Epoch 115, Train Loss: 3.2654985796956337, Val Loss: 3.060667237841097\n",
      "Epoch 116, Train Loss: 3.2536576305971985, Val Loss: 3.2072580111654183\n",
      "Epoch 117, Train Loss: 3.1978732543011588, Val Loss: 3.2429530495091488\n",
      "Epoch 118, Train Loss: 3.2252665163582837, Val Loss: 3.2106425547062005\n",
      "save model\n",
      "Epoch 119, Train Loss: 3.2170544617345644, Val Loss: 2.9996022712019155\n",
      "save model\n",
      "Epoch 120, Train Loss: 3.1632083138661407, Val Loss: 2.8834692938883504\n",
      "Epoch 121, Train Loss: 3.1546274492431383, Val Loss: 2.9934879980589213\n",
      "Epoch 122, Train Loss: 3.1842409008217656, Val Loss: 3.124431219764222\n",
      "Epoch 123, Train Loss: 3.1356942169835875, Val Loss: 3.0339078567081823\n",
      "Epoch 124, Train Loss: 3.15281064889421, Val Loss: 3.2452774352597116\n",
      "Epoch 125, Train Loss: 3.0977654019160252, Val Loss: 2.988502599242935\n",
      "Epoch 126, Train Loss: 3.138073426510001, Val Loss: 3.1599090538526835\n",
      "Epoch 127, Train Loss: 3.121841148951064, Val Loss: 2.9888177760561607\n",
      "Epoch 128, Train Loss: 3.090339599693171, Val Loss: 3.1100508624449708\n",
      "Epoch 129, Train Loss: 3.08726784813853, Val Loss: 3.022873031913786\n",
      "Epoch 130, Train Loss: 3.0852875444180796, Val Loss: 3.2914626956882334\n",
      "Epoch 131, Train Loss: 3.0335109112152994, Val Loss: 3.209898923572741\n",
      "Epoch 132, Train Loss: 3.0526790014370713, Val Loss: 3.326093968592192\n",
      "save model\n",
      "Epoch 133, Train Loss: 3.052300236664058, Val Loss: 2.879687972534868\n",
      "Epoch 134, Train Loss: 3.018987850564294, Val Loss: 3.080750434470356\n",
      "Epoch 135, Train Loss: 3.020352916637724, Val Loss: 3.132731243183738\n",
      "Epoch 136, Train Loss: 3.0106509664567445, Val Loss: 3.027612869900868\n",
      "Epoch 137, Train Loss: 3.0049333464650427, Val Loss: 2.955229271623425\n",
      "Epoch 138, Train Loss: 2.988979052998531, Val Loss: 2.9779140070865027\n",
      "Epoch 139, Train Loss: 3.0013875343809566, Val Loss: 2.9771312891988826\n",
      "Epoch 140, Train Loss: 2.962381494394406, Val Loss: 3.1032313413189767\n",
      "Epoch 141, Train Loss: 2.980803578568303, Val Loss: 2.9329997941963653\n",
      "save model\n",
      "Epoch 142, Train Loss: 2.956788258173476, Val Loss: 2.8595860972440335\n",
      "Epoch 143, Train Loss: 2.919061136844268, Val Loss: 2.959509635330143\n",
      "Epoch 144, Train Loss: 2.9311908493480923, Val Loss: 2.866808553835503\n",
      "Epoch 145, Train Loss: 2.9043139202325414, Val Loss: 2.944316349531475\n",
      "save model\n",
      "Epoch 146, Train Loss: 2.9278075733942965, Val Loss: 2.858544395382243\n",
      "Epoch 147, Train Loss: 2.9404320641042796, Val Loss: 2.98466924527534\n",
      "Epoch 148, Train Loss: 2.9021982005450515, Val Loss: 2.992401981712284\n",
      "Epoch 149, Train Loss: 2.8766001618556896, Val Loss: 2.9504289671890715\n",
      "Epoch 150, Train Loss: 2.8798544691197545, Val Loss: 3.0248852551431584\n",
      "Epoch 151, Train Loss: 2.857431748821147, Val Loss: 3.1417322898269595\n",
      "save model\n",
      "Epoch 152, Train Loss: 2.9136593410659533, Val Loss: 2.831808606484779\n",
      "Epoch 153, Train Loss: 2.8159711767938846, Val Loss: 2.938100244765891\n",
      "Epoch 154, Train Loss: 2.8341558062381824, Val Loss: 2.8428186935589728\n",
      "Epoch 155, Train Loss: 2.845162385377924, Val Loss: 2.9893403680701005\n",
      "Epoch 156, Train Loss: 2.8376970595395714, Val Loss: 2.970249705744865\n",
      "Epoch 157, Train Loss: 2.811760412100469, Val Loss: 2.987477741295234\n",
      "Epoch 158, Train Loss: 2.8113815855281623, Val Loss: 2.9799192086198274\n",
      "Epoch 159, Train Loss: 2.7693156927699323, Val Loss: 2.8548987857381203\n",
      "Epoch 160, Train Loss: 2.7832280743571007, Val Loss: 3.0367706776561594\n",
      "Epoch 161, Train Loss: 2.78902292341368, Val Loss: 3.062919648966395\n",
      "Epoch 162, Train Loss: 2.7552894259097687, Val Loss: 3.0496978060643474\n",
      "Epoch 163, Train Loss: 2.780894960319647, Val Loss: 3.1091818863287903\n",
      "Epoch 164, Train Loss: 2.767517612070219, Val Loss: 2.9095520112747537\n",
      "Epoch 165, Train Loss: 2.7438461410450636, Val Loss: 2.957369527870551\n",
      "Epoch 166, Train Loss: 2.7458325667361336, Val Loss: 2.958888570168861\n",
      "Epoch 167, Train Loss: 2.7313196637141655, Val Loss: 2.9213343164078274\n",
      "Epoch 168, Train Loss: 2.7364586654068535, Val Loss: 2.877885253805863\n",
      "Epoch 169, Train Loss: 2.6669151560532, Val Loss: 2.998925344836443\n",
      "Epoch 170, Train Loss: 2.674596880769131, Val Loss: 3.0658552216407946\n",
      "Epoch 171, Train Loss: 2.684756608388414, Val Loss: 2.897953951269164\n",
      "Epoch 172, Train Loss: 2.698557838435951, Val Loss: 2.8355131516779277\n",
      "save model\n",
      "Epoch 173, Train Loss: 2.6539821502055085, Val Loss: 2.8086079318720594\n",
      "Epoch 174, Train Loss: 2.6911345710315464, Val Loss: 2.842970095630875\n",
      "save model\n",
      "Epoch 175, Train Loss: 2.6584977888163164, Val Loss: 2.753715214872719\n",
      "save model\n",
      "Epoch 176, Train Loss: 2.6386087045509945, Val Loss: 2.6619477921858765\n",
      "Epoch 177, Train Loss: 2.628226390842613, Val Loss: 3.028332925380621\n",
      "Epoch 178, Train Loss: 2.648268009229684, Val Loss: 2.8467173683912232\n",
      "Epoch 179, Train Loss: 2.6484991601321486, Val Loss: 2.8211264009762527\n",
      "Epoch 180, Train Loss: 2.6151333279190703, Val Loss: 2.7659410171042706\n",
      "Epoch 181, Train Loss: 2.6231382729119335, Val Loss: 2.839357822909391\n",
      "Epoch 182, Train Loss: 2.5880528303369816, Val Loss: 2.766682921943808\n",
      "Epoch 183, Train Loss: 2.606790594376281, Val Loss: 2.8596884285596977\n",
      "Epoch 184, Train Loss: 2.5659690239938233, Val Loss: 2.8708292792614243\n",
      "Epoch 185, Train Loss: 2.591712361698869, Val Loss: 2.919551294549067\n",
      "Epoch 186, Train Loss: 2.6102037223313146, Val Loss: 3.0172830250926483\n",
      "Epoch 187, Train Loss: 2.6083498121804274, Val Loss: 2.7961323220927015\n",
      "Epoch 188, Train Loss: 2.564612325714223, Val Loss: 2.7447342428945958\n",
      "Epoch 189, Train Loss: 2.5686454041732407, Val Loss: 3.3680811887396906\n",
      "Epoch 190, Train Loss: 2.5311057279299494, Val Loss: 2.793653514600338\n",
      "Epoch 191, Train Loss: 2.510146476234851, Val Loss: 2.960791102029327\n",
      "Epoch 192, Train Loss: 2.504196076612592, Val Loss: 2.764400601387024\n",
      "Epoch 193, Train Loss: 2.5025198069576438, Val Loss: 2.7980891882925105\n",
      "Epoch 194, Train Loss: 2.5165673236467847, Val Loss: 2.8027977773121426\n",
      "save model\n",
      "Epoch 195, Train Loss: 2.4958725445440124, Val Loss: 2.6487883903030167\n",
      "Epoch 196, Train Loss: 2.4908683074568105, Val Loss: 2.7416084659727\n",
      "Epoch 197, Train Loss: 2.5188799685015337, Val Loss: 2.922450492256566\n",
      "Epoch 198, Train Loss: 2.4843175162830113, Val Loss: 2.7327081314603188\n",
      "Epoch 199, Train Loss: 2.4836168715145797, Val Loss: 2.780362077673575\n",
      "Epoch 200, Train Loss: 2.4510585790897514, Val Loss: 2.721783696260667\n",
      "Epoch 201, Train Loss: 2.502620959381678, Val Loss: 2.820589728821489\n",
      "Epoch 202, Train Loss: 2.435449727609068, Val Loss: 2.748240332406266\n",
      "Epoch 203, Train Loss: 2.431986010523521, Val Loss: 2.7522765985108855\n",
      "Epoch 204, Train Loss: 2.4330254073422326, Val Loss: 2.74831943180328\n",
      "Epoch 205, Train Loss: 2.4337790590449857, Val Loss: 3.0540575174460733\n",
      "Epoch 206, Train Loss: 2.439955033318268, Val Loss: 2.80446508907734\n",
      "Epoch 207, Train Loss: 2.4329398150224564, Val Loss: 2.7713307070552853\n",
      "Epoch 208, Train Loss: 2.4129355538839077, Val Loss: 2.750355400761267\n",
      "Epoch 209, Train Loss: 2.437827060761312, Val Loss: 2.817907317240435\n",
      "Epoch 210, Train Loss: 2.3883889952464084, Val Loss: 2.723214084044435\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 8\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, criterion, optimizer, epochs)\u001b[0m\n\u001b[0;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m      7\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m      9\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32), labels\u001b[38;5;241m.\u001b[39mto(device, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     10\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m--> 630\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_name):\n\u001b[0;32m    631\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m             \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    633\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\autograd\\profiler.py:492\u001b[0m, in \u001b[0;36mrecord_function.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    491\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 492\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecord \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_record_function_enter_new\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    493\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\_ops.py:502\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    498\u001b[0m     \u001b[38;5;66;03m# overloading __call__ to ensure torch.ops.foo.bar()\u001b[39;00m\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;66;03m# is still callable from JIT\u001b[39;00m\n\u001b[0;32m    500\u001b[0m     \u001b[38;5;66;03m# We save the function ptr as the `op` attribute on\u001b[39;00m\n\u001b[0;32m    501\u001b[0m     \u001b[38;5;66;03m# OpOverloadPacket to access it here.\u001b[39;00m\n\u001b[1;32m--> 502\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_op(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs \u001b[38;5;129;01mor\u001b[39;00m {})\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model(model=model, train_loader=train_dataloader, val_loader=valid_dataloader, criterion=criterion, optimizer=optimizer, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
